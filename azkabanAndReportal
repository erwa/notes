$ curl -k https://<host>:<port> -d "username=<username>&password=<password>&action=login"
{
  "status" : "success",
  "session.id" : "<session.id>"
}

$ curl -k https://<host>:<port>/manager -G -d "project=reportal-ahsu-Reportal-Monitoring-Report&flow=data-collector&ajax=fetchFlowExecutions&start=0&length=1" -b azkaban.browser.session.id=<session.id>
{
  "executions" : [ {
    "startTime" : 1396998055295,
    "submitUser" : "ahsu",
    "status" : "SUCCEEDED",
    "submitTime" : 1396998055231,
    "execId" : 44607,
    "projectId" : 909,
    "endTime" : 1396998377787,
    "flowId" : "data-collector"
  } ],
  "total" : 7,
  "project" : "reportal-ahsu-Reportal-Monitoring-Report",
  "length" : 1,
  "from" : 0,
  "flow" : "data-collector",
  "projectId" : 909
}

Azkaban servlet routes registered in AzkabanWebServer.java:main

Can just set hadoop.home and hive.home in common.properties and commonprivate.properties and don't need to redefine these properties in all the jobtype subdirectories.

# On OS X, to run a local SMTP mail server for local testing, run
sudo postfix start

# For Pig job type, you can override Hadoop confs by setting setting jvm.args. Example:
-Dmapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec
