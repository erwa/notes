curl -k https://<host>:<port> -d "username=<username>&password=<password>&action=login"
{
  "status" : "success",
  "session.id" : "<session.id>"
}

$ curl -k https://<host>:<port>/manager -G -d "project=project-name&flow=flow-name&ajax=fetchFlowExecutions&start=0&length=1" -b azkaban.browser.session.id=<session.id>
{
  "executions" : [ {
    "startTime" : 1396998055295,
    "submitUser" : "ahsu",
    "status" : "SUCCEEDED",
    "submitTime" : 1396998055231,
    "execId" : 44607,
    "projectId" : 909,
    "endTime" : 1396998377787,
    "flowId" : "flow-name"
  } ],
  "total" : 7,
  "project" : "project-name",
  "length" : 1,
  "from" : 0,
  "flow" : "flow-name",
  "projectId" : 909
}

Azkaban servlet routes registered in AzkabanWebServer.java:main

Can just set hadoop.home and hive.home in common.properties and commonprivate.properties and don't need to redefine these properties in all the jobtype subdirectories.

# On OS X, to run a local SMTP mail server for local testing, run
sudo postfix start

# For Pig job type, you can override Hadoop confs by setting setting jvm.args. Example:
-Dmapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec

# reload jobtype plugins without restarting executor server
curl http://localhost:<port>/executor?action=reloadJobTypePlugins

# get project permissions
curl -k https://<host>:<port>/manager -G -d "ajax=getPermissions&project=project-name" -b azkaban.browser.sesson.id=<session.id>
{
  "project" : "project-name",
  "permissions" : [ {
    "username" : "ahsu",
    "permission" : [ "ADMIN" ]
  } ],
  "projectId" : 71
}

# execute flow
curl -k https://<host>:<port>/executor -G -d "ajax=executeFlow&project=project-name&flow=flow-name" -b azkaban.browser.session.id=<session.id>
{
  "message" : "Execution submitted successfully with exec id 32285",
  "project" : "project-name",
  "flow" : "flow-name",
  "execid" : 32285
}

# The number of flows in a project is equal to the number of end nodes (sinks). If you have a dependency graph like
  A
 / \
B   C
# when you upload the project, you will get two flows: B and C

# Can also use &session.id=<session.id> instead of setting the azkaban.browser.session.id cookie

# Get flow status
curl -k https://<host>:<port>/executor -d "ajax=fetchexecflowupdate&execid=<exec.id>&lastUpdateTime=-1&session.id=<session.id>"

{
  "id" : "test",
  "startTime" : 1408152686395,
  "attempt" : 0,
  "status" : "RUNNING",
  "updateTime" : 1408152686423,
  "nodes" : [ {
    "attempt" : 0,
    "startTime" : 1408152686412,
    "id" : "test",
    "updateTime" : 1408152686420,
    "status" : "RUNNING",
    "endTime" : -1
  } ],
  "flow" : "test",
  "endTime" : -1
}

# Kill flow
curl -k https://<host>:<port>/executor -d "ajax=cancelFlow&execid=<exec.id>&session.id=<session.id>

# Scheduling a flow
# Login first (see above)

curl -k https://<host>:<port>/schedule -d "ajax=scheduleFlow&projectName=<projectName>&flow=<flowName>&projectId=<projectId>&scheduleTime=12,00,pm,PDT&scheduleDate=07/22/2014" -b azkaban.browser.session.id=<session.id>
{
  "message" : "<projectName>.<flowName> scheduled.",
  "status" : "success"
}

# Get all projects
curl -k https://<host>:<port>/index?all -b azkaban.browser.session.id=<session.id>

# If you're getting a Jetty NOT_FOUND error when you try to load Azkaban,
# it probably means one of your viewer plugins is not configured properly.

# When installing Reportal, make sure to include velocity-tools-2.0.jar and slf4j-log4j12-1.6.4.jar
# If you're using Hadoop, make sure to include the hadoop-core jar and add the Hadoop conf directory to your classpath.

# Fetch giant log
https://<host>:<port>/executor?execid=<execid>&jobId=<jobid>&ajax=fetchExecJobLogs&offset=0&length=2147483647&attempt=0

# For Azkaban, to build just the jars (and skip the tests), just run
./gradlew jar

# Create a project
curl -k https://<host>:<port>/manager -d "action=create&name=<name>&description=<description>" -b azkaban.browser.session.id=<session.id>

# Data trigger
# Azkaban currently does not support data triggers
# However, one workaround is to use Azkaban's "retries" feature with "retry.backoff"

# On OSX, tests may fail without setting
export JAVA_TOOL_OPTIONS=-Dfile.encoding=UTF-8
# Fixed by https://github.com/azkaban/azkaban/pull/343

# Do not run Azkaban as root or else many default environment variables like
# JAVA_HOME and HADOOP_HOME will probably not be set and jobs will fail

# Azkaban documentation source resides in gh-pages branch:
# https://github.com/azkaban/azkaban/tree/gh-pages
# To use Jekyll and GitHub Pages, see https://help.github.com/articles/using-jekyll-with-pages/
gem install bundler

# Create 'Gemfile' in repo with these contents:
source 'https://rubygems.org'
gem 'github-pages'

# Then run
bundle install
bundle exec jekyll serve
# View the page at http://localhost:4000
# If port is already in use, you can launch on different port
jekyll serve -P 4001
